{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC421: Homework #1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Problem 2.10: pg 63\n",
    "\n",
    "**2.10** Consider a modified version of the vacuum environment in Exercise 2.8, in which the agent is penalized one point for each movement.\n",
    "\n",
    "**a.** Can a simple reflex agent be perfectly rational for this environment? Explain.  \n",
    "**b.** What about a reflex agent with state? Design such an agent.  \n",
    "**c.** How do your answers to a and b change if the agent’s percepts give it the clean/dirty status of every square in the environment?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** A simple reflex agent cannot be perfectly rational\n",
    "in this environment. Since the agent is penalized one point for each movement and the reflex agent has no history of what has already done, then the agent will eventually take unnecessary movements. Take for example the agent has cleaned both room A and room B and ends up in room B. At this point, it does not have any historical knowledge that it did this and will turn Left (according to the function map) and check if Room A again and repeat.   \n",
    "\n",
    "\n",
    "**b)**  A reflex agent with state can be perfectly rational in this environment. Since the reflex agent is able to maintain an internal model/state representing the vacuum world, it can map those states to certain reflexes. \n",
    "```\n",
    "Function state-reflex-vacuum-agent([location,status]) returns an action\n",
    "    if state[location] == Clean :\n",
    "       if location = A and status[B] == Dirty:\n",
    "           return Right\n",
    "       elif location = B and status[A] == Dirty:\n",
    "           return Left\n",
    "       else:\n",
    "           return Done\n",
    "\telse:\n",
    "        state[location] == Clean\n",
    "        return suck\n",
    "\t\n",
    "```\n",
    "\n",
    "**c)** If the agent’s percepts give it the clean/dirty of every square in the environment then it can perform perfectly rational. The simple reflex agent can have a reflex for every scenario based on the precepts it was given. The simple reflex agent with state can also update it's internal state based on the perepts given and execute it's reflexes rationally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.15 (a,b): pg 116\n",
    "\n",
    "**3.15** Consider a state space where the start state is number 1 and each state k has two successors: numbers 2k and 2k + 1.    \n",
    "\n",
    "**a.** Draw the portion of the state space for states 1 to 15.  \n",
    "**b.** Suppose the goal state is 11. List the order in which nodes will be visited for breadth-first search, depth-limited search with limit 3, and iterative deepening search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binarytree import Node, tree\n",
    "\n",
    "root = Node(1)    \n",
    "root.left = Node(2)\n",
    "root.right = Node(3)\n",
    "root.left.left = Node(4)\n",
    "root.left.right = Node(5)\n",
    "root.right.left = Node(6)\n",
    "root.right.right = Node(7)\n",
    "root.left.left.left = Node(8)\n",
    "root.left.left.right = Node(9)\n",
    "root.left.right.left = Node(10)\n",
    "root.left.right.right = Node(11)\n",
    "root.right.left.left = Node(12)\n",
    "root.right.left.right = Node(13)\n",
    "root.right.right.left = Node(14)\n",
    "root.right.right.right = Node(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ________1________\n",
      "       /                 \\\n",
      "    __2___             ___3___\n",
      "   /      \\           /       \\\n",
      "  4       _5        _6        _7\n",
      " / \\     /  \\      /  \\      /  \\\n",
      "8   9   10   11   12   13   14   15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**   \n",
    "    **Breadth-first-search:** 1,2,3,4,5,6,7,8,9,10,11  \n",
    "    **Depth-Limited-search:** 1,2,4,8,9,5,10,11  \n",
    "    **Iterative-Deepening:**   \n",
    "       1st Iteration: 1  \n",
    "       2nd Iteration: 1,2,3  \n",
    "       3rd Iteration: 1,2,4,5,3,6,7  \n",
    "       4th Iteration: 1,2,4,8,9,5,10,11  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.18: pg 117\n",
    "\n",
    "**3.18** Describe a state space in which iterative deepening search performs much worse than depth-first search (for example, O(n2) vs. O(n)).\n",
    "\n",
    "m = maximum depth\n",
    "\n",
    "A state space where the depth is large and the goal is at the bottom is a space where iterative deepining search will perform much worse than depth-first search. Depth first search will perform better because it will reach the bottom of the state space very quickly. Iterative deepening search will reach the bottom on it's m'th iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (robenv)",
   "language": "python",
   "name": "robenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
